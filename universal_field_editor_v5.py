# -*- coding: utf-8 -*-
"""
Universal Field Editor Script for Dataverse

A script for bulk editing metadata fields in Dataverse datasets using CSV files.
This script connects to a Dataverse instance via its API and updates dataset metadata
based on values specified in CSV files.

Automatically generated by Colab and enhanced with proper documentation.
"""

#!pip install pyDataverse

# Standard library imports
from datetime import datetime
import sys
import time
import csv
import json

# Third-party imports
import pandas as pd
import requests

# Dataverse API imports
# Documentation: https://pydataverse.readthedocs.io/en/latest/
import pyDataverse.utils as utils
from pyDataverse.api import NativeApi, DataAccessApi


# ============================================================================
# CONFIGURATION SETTINGS
# ============================================================================

# CSV file directories - list format allows multiple metadata blocks
"""
If you have more than one CSV file, create a list of directories
file directory example:

file_directory = ['/Citation Fields CSV - Citation.csv',
                  '/Citation Fields CSV - Geospatial.csv',
                  '/Citation Fields CSV - Social Science and Humanities.csv',
                  '/Citation Fields CSV - Astronomy and Astrophysics.csv',
                  '/Citation Fields CSV - Journal.csv',
                  '/Citation Fields CSV - Life Sciences.csv',
                  '/Citation Fields CSV - Computational Workflow.csv',
                  '/Citation Fields CSV - 3D Objects.csv']
"""
file_directory = []

# API credentials and endpoint configuration
api_token_origin = "API KEY HERE"           # Dataverse API token
url_base_origin = 'BASE URL HERE'           # Dataverse base URL (e.g., https://demo.borealisdata.ca)


# API headers and client initialization
headers_origin = {'X-Dataverse-key': api_token_origin}
api_origin = NativeApi(url_base_origin, api_token_origin)
data_api_origin = DataAccessApi(url_base_origin, api_token_origin)



# ============================================================================
# CORE FUNCTIONS
# ============================================================================

def check_lock(dataset_id, lock_status):
    """
    NOT CURRENTLY IN USE - FOR FUTURE VERSION

    Check if a dataset is locked and wait for lock to be released.

    Dataset locks can occur during various operations like file ingestion or
    publishing. This function polls the lock status until the dataset is unlocked.

    Args:
        dataset_id (str): The ID of the dataset to check

    Returns:
        bool: True if dataset is unlocked, False if an error occurred

    Note:
        Documentation on dataset locks:
        https://guides.dataverse.org/en/latest/api/native-api.html#dataset-locks
    """
    time_start = datetime.now()
    print("Start check_lock")

    try:
        url = f"{url_base_origin}/api/datasets/{dataset_id}/locks"
        lock = requests.get(url, headers=headers_origin)

        if lock.status_code == 503:
            print("503 - Server is unavailable")
            sys.exit()


        if lock_status != 0:
            attempt_count = 0
            while len(lock.json()['data']) > 0:
                print(f"Lock {attempt_count} times {dataset_id} {lock.json()}")
                print(lock.json())
                time.sleep(10)
                attempt_count += 1

                lock = requests.get(url, headers=headers_origin)
                if lock.status_code == 503:
                    print("503 - Server is unavailable")
                    sys.exit()

                if lock.status_code != 200:
                    print(f"check_lock func: lock status {lock.status_code} for {dataset_id}")
                    return False

        else:
              if len(lock.json()['data']) > 0:
                  print(f"check_lock func: lock status {lock.status_code} for {dataset_id}")
                  return False


    except Exception as e:
        print(f"check_lock Error: {str(e)}, dataset {dataset_id}")
        return False

    time_end = datetime.now()
    elapsed_time = (time_end - time_start).total_seconds()
    print(f"Dataset {dataset_id} was locked {elapsed_time} sec")

    return True





def update_metadata(latest_version, row, doi, header, directory, master_list, block):
    """
    Update dataset metadata by parsing CSV row values and pushing changes via API.

    This function processes metadata fields from a CSV row, formats them appropriately
    (primitive or compound), and sends updates to the Dataverse API.

    Args:
        latest_version (dict): Latest version metadata from Dataverse
        row (dict): Current CSV row containing update values
        doi (str): Dataset DOI
        header (list): CSV column headers
        directory (dict): Field definitions for the metadata block
        master_list (list): Contains [primitive_fields, compound_fields, controlled_vocab_fields]
        block (str): Metadata block name (e.g., 'citation', 'socialscience')
    """
    metadata_blocks = latest_version['metadataBlocks']


    if block not in metadata_blocks:
        generated_record = metadatablock_generator(block)
        metadata_blocks[block] = generated_record[block]
        fields = metadata_blocks[block]['fields']

    else:
        fields = metadata_blocks[block]['fields']

    print('%%%%%%%%%%%%%%%%%%%%%%%%%%%%')
    print(fields)
    print('%%%%%%%%%%%%%%%%%%%%%%%%%%%%')

    existing_fields = [field for field in fields]
    existing_field_names = [field['typeName'] for field in fields]
    print(existing_fields)
    print(existing_field_names)

    field_index = 0
    for change_area in header:
        field_name = change_area.split(":")[0]
        print(field_name)

        # Handle new fields not in existing record
        if field_name not in existing_field_names and field_name != 'doi':
            print(f"{field_name} -- NOT IN EXISTING RECORD")

            if row[change_area] != '':
                print(f'RECORD TO ADD: {row[change_area]}')
                field = directory[field_name]

                # Process primitive fields
                if field_name in master_list[0]:
                    output = primitive_formatter(change_area, row, field)

                    if output == '':
                        field_index += 1
                        continue
                    else:
                        API_push(field, doi)


                # Process compound fields
                elif field_name in master_list[1]:
                    output = compound_formatter(change_area, row)
                    print(f'OUTPUT: {output}')

                    if output == False:
                        print()
                        field_index += 1
                        continue

                    else:
                        if field['multiple'] == True:
                            field['value'] = output

                        else:
                            field['value'] = output[0]

                    API_push(field, doi)

            else:
                print('-- NO RECORD TO ADD --')
                print()
                continue

        # Handle existing fields
        elif field_name in existing_field_names:
            current_field = existing_fields[field_index]
            print(f'Change_area = {change_area}')
            print(f'Fields = {current_field["typeName"]}')

            # Update primitive fields
            if current_field['typeName'] == change_area and current_field['typeName'] in master_list[0]:
                output = primitive_formatter(change_area, row, current_field)
                #print(f' {output}')

                if output == '':
                    field_index += 1
                    continue
                else:
                    API_push(current_field, doi)

            # Update compound fields
            elif current_field['typeName'] in change_area and current_field['typeName'] in master_list[1]:
                field_format = compound_formatter(change_area, row)
                print(f'FIELD FORMAT: {field_format}')

                if field_format == False:
                    print()
                    field_index += 1
                    continue

                else:
                    if current_field['multiple']:
                        current_field['value'] = field_format

                    else:
                        current_field['value'] = field_format[0]
                        #print(f'FIELD FORMAT: {field_format[0]}')

                    API_push(current_field, doi)

            field_index += 1





def primitive_formatter(change_area, row, field):
    """
    Format primitive metadata field values for updates.

    Handles single or multiple values, including splitting on '+' delimiter
    for multiple values in a single CSV cell.

    Args:
        change_area (str): Name of the field being updated
        row (dict): CSV row containing the new values
        field (dict): Field definition from the directory

    Returns:
        dict or str: Formatted field ready for API update
    """
    current_value = field['value']
    print(f"Unit to replace = {current_value}")

    # Handle multiple values separated by '+'
    if '+' in row[change_area]:
        new_values = row[change_area].split('+')
        print(f'New unit (plural): {new_values}')
        return record_check(new_values, field)

    # Handle single value
    else:
        if field['multiple']:
            new_values = [row[change_area]]
            print(f'New unit (singular): {new_values}')
            return record_check(new_values, field)
        else:
            new_value = row[change_area]
            print(f'New unit (singular): {new_value}')
            return record_check(new_value, field)





def record_check(new_value, field):
    """
    Validate and assign new value to field.

    Helper function to avoid code duplication in primitive_formatter.
    Checks if the new value is empty and handles field assignment.

    Args:
        new_value: The new value to assign (string or list)
        field (dict): Field dictionary to update

    Returns:
        dict or str: Updated field or empty string if no update needed
    """
    if new_value == [''] or new_value == '':
        print('NOT UPDATED IN THE RECORD')
        return ''
    else:
        field['value'] = new_value
        return field





def compound_formatter(header, row):
    """
    Format compound metadata field values for updates.

    Compound fields contain multiple primitive sub-fields. This function
    parses the header to identify sub-fields and formats the values accordingly.

    Args:
        header (str): CSV column header containing field and sub-field definitions
        row (dict): CSV row with the values to update

    Returns:
        list or bool: List of formatted compound values, or False if no update needed
    """
    first_list = header.split(":")

    if ';' in first_list[1]:
        compound_list = [i.strip() for i in first_list[1].split(';')]

    else:
        compound_list = [first_list[1].strip()]


    citation_dictionary = {}
    citation_dictionary[f'{first_list[0]}'] = compound_list
    print(f"Citation dictionary = {citation_dictionary}")
    print(f'Compound list = {compound_list}')


    if row[header] != '':

        if '+' in row[header]:
            val_list = row[header].split('+')
            value_list = []
            for i in val_list:
                value = i.split(';')
                value_list.append(value)

        else:
            value_list = row[header].split(';')

        print(f"Value list = {value_list}")

    else:
        value_list = []

    counter = 0
    list_of_list = []

    for types in citation_dictionary[f'{first_list[0]}']:
        populated_dictionary = {}

        if counter >= len(value_list):
            break

        if value_list == [''] or value_list == []:
          counter += 1
          continue

        else:
            list_of_list = []
            counter_2 = 0

            for items in value_list:
                val_setup = {}

                if type(items) is list:
                    count = 0

                    for i in items:
                        i = i.strip()
                        primitive_dictionary = {}
                        primitive_dictionary['typeName'] = compound_list[count].strip()
                        primitive_dictionary['multiple'] = False
                        primitive_dictionary['typeClass'] = "primitive"
                        primitive_dictionary['value'] = i

                        val_setup[compound_list[count].strip()] = primitive_dictionary
                        count += 1

                else:
                    items = items.strip()
                    primitive_dictionary = {}
                    primitive_dictionary['typeName'] = compound_list[counter_2].strip()
                    primitive_dictionary['multiple'] = False
                    primitive_dictionary['typeClass'] = "primitive"
                    primitive_dictionary['value'] = items

                    populated_dictionary[compound_list[counter_2].strip()] = primitive_dictionary
                    counter_2 += 1

                if val_setup != {}:
                  list_of_list.append(val_setup)

        if val_setup == {}:
            list_of_list.append(populated_dictionary)

        counter += 1

    if list_of_list == []:
        print("NOT UPDATED IN THE RECORD")
        return False

    else:
        return list_of_list





def file_loader():
    """
    Main entry point for processing CSV files and updating dataset metadata.

    Iterates through configured CSV files, reads each row, and updates
    corresponding dataset metadata in Dataverse.
    """
    lock_status = 0
    compilation_skipped_entries = []

    for csv_path in file_directory:
        df = pd.read_csv(csv_path)
        headers = list(df.columns)
        print(headers)

        # Get metadata block configuration
        block_info = xml_selecter(headers)
        field_directory = block_info[0]
        block_name = block_info[1]
        master_lists = block_info[2]

        with open(csv_path, newline='', encoding='utf-8-sig') as csvfile:
            reader = csv.DictReader(csvfile)

            for row in reader:
                print(row)
                doi = row[headers[0]]

                # Standardize DOI format
                if 'https://doi.org/' in doi:
                    doi = doi.replace('https://doi.org/', 'doi:')
                print(doi)

                resp = api_origin.get_dataset(doi, version="2.0")
                print(resp.json())

                if resp.status_code == 200:
                    dataset_id = resp.json()['data']['id']
                    latest_version = resp.json()['data']['latestVersion']

                    status = check_lock(dataset_id, lock_status)

                    if status == True:
                        # Update metadata
                        update_metadata(latest_version, row, doi, headers, field_directory, master_lists, block_name)

                    else:
                        # Document data for update at end of task
                        skipped_entry_data = [latest_version, row, doi, headers, field_directory, master_lists, block_name, dataset_id]
                        compilation_skipped_entries.append(skipped_entry_data)
                        print(f'SKIPPED ENTRY DUE TO LOCK ISSUE - WILL TRY AGAIN AT END OF TASK: {skipped_entry_data}')
                        print()
                        continue

                    # Optional: Auto-publish dataset
                    # publish_dataset(doi)


    if len(compilation_skipped_entries) > 0:
        print()
        print('UPDATING METADATA OF LOCKED DATASETS - THIS PROCESS MAY TAKE A WHILE IF THE DATASET IS STILL LOCKED')
        print()

        for locked_sets in compilation_skipped_entries:
            lock_status = 1

            latest_version = locked_sets[0]
            row = locked_sets[1]
            doi = locked_sets[2]
            headers = locked_sets[3]
            field_directory = locked_sets[4]
            master_lists = locked_sets[5]
            block_name = locked_sets[6]
            dataset_id = locked_sets[7]

            status = check_lock(dataset_id, lock_status)

            if status == True:
                update_metadata(latest_version, row, doi, headers, field_directory, master_lists, block_name)

                # Optional: Auto-publish dataset
                # publish_dataset(doi)







def xml_selecter(headers):
    """
    Select and configure metadata block based on CSV headers.

    Determines which metadata block (citation, socialscience, etc.) to use
    based on markers in the CSV headers and returns the corresponding
    field definitions and configurations.

    Args:
        headers (list): CSV column headers

    Returns:
        list: [field_directory, block_name, master_lists]
    """

    # Citation metadata block configuration
    if 'citation' in headers:
        field_directory = {
            'title': {"typeName": "title", "multiple": False, "typeClass": "primitive", "value": ""},
            'subtitle': {"typeName": "subtitle", "multiple": False, "typeClass": "primitive", "value": ""},
            'alternativeTitle': {"typeName": "alternativeTitle", "multiple": True, "typeClass": "primitive", "value": [""]},
            'otherId': {"typeName": "otherId", "multiple": True, "typeClass": "compound", "value": [""]},
            'author': {"typeName": "author", "multiple": True, "typeClass": "compound", "value": [""]},
            'datasetContact': {"typeName": "datasetContact", "multiple": True, "typeClass": "compound", "value": [""]},
            'dsDescription': {"typeName": "dsDescription", "multiple": True, "typeClass": "compound", "value": [""]},
            'subject': {"typeName": "subject", "multiple": True, "typeClass": "controlledVocabulary", "value": [""]},
            'keyword': {"typeName": "keyword", "multiple": True, "typeClass": "compound", "value": [""]},
            'topicClassification': {"typeName": "topicClassification", "multiple": True, "typeClass": "compound", "value": [""]},
            'publication': {"typeName": "publication", "multiple": True, "typeClass": "compound", "value": [""]},
            'notesText': {"typeName": "notesText", "multiple": False, "typeClass": "primitive", "value": ""},
            'language': {"typeName": "language", "multiple": True, "typeClass": "controlledVocabulary", "value": [""]},
            'producer': {"typeName": "producer", "multiple": True, "typeClass": "compound", "value": [""]},
            'productionDate': {"typeName": "productionDate", "multiple": False, "typeClass": "primitive", "value": ""},
            'productionPlace': {"typeName": "productionPlace", "multiple": True, "typeClass": "primitive", "value": [""]},
            'contributor': {"typeName": "contributor", "multiple": True, "typeClass": "compound", "value": [""]},
            'grantNumber': {"typeName": "grantNumber", "multiple": True, "typeClass": "compound", "value": [""]},
            'distributor': {"typeName": "distributor", "multiple": True, "typeClass": "compound", "value": [""]},
            'distributionDate': {"typeName": "distributionDate", "multiple": False, "typeClass": "primitive", "value": ""},
            'depositor': {"typeName": "depositor", "multiple": False, "typeClass": "primitive", "value": ""},
            'dateOfDeposit': {"typeName": "dateOfDeposit", "multiple": False, "typeClass": "primitive", "value": ""},
            'timePeriodCovered': {"typeName": "timePeriodCovered", "multiple": True, "typeClass": "compound", "value": [""]},
            'dateOfCollection': {"typeName": "dateOfCollection", "multiple": True, "typeClass": "compound", "value": [""]},
            'kindOfData': {"typeName": "kindOfData", "multiple": True, "typeClass": "primitive", "value": [""]},
            'series': {"typeName": "series", "multiple": True, "typeClass": "compound", "value": [""]},
            'software': {"typeName": "software", "multiple": True, "typeClass": "compound", "value": [""]},
            'relatedMaterial': {"typeName": "relatedMaterial", "multiple": True, "typeClass": "primitive", "value": [""]},
            'relatedDatasets': {"typeName": "relatedDatasets", "multiple": True, "typeClass": "primitive", "value": [""]},
            'otherReferences': {"typeName": "otherReferences", "multiple": True, "typeClass": "primitive", "value": [""]},
            'dataSources': {"typeName": "dataSources", "multiple": True, "typeClass": "primitive", "value": [""]},
            'originOfSources': {"typeName": "originOfSources", "multiple": False, "typeClass": "primitive", "value": ""},
            'characteristicOfSources': {"typeName": "characteristicOfSources", "multiple": False, "typeClass": "primitive", "value": ""},
            'accessToSources': {"typeName": "accessToSources", "multiple": False, "typeClass": "primitive", "value": ""}
        }
        block_name = 'citation'


    # Social science metadata block configuration
    elif 'socialscience' in headers:
        field_directory = {
            'unitOfAnalysis': {"typeName": "unitOfAnalysis", "multiple": True, "typeClass": "primitive", "value": [""]},
            'universe': {"typeName": "universe", "multiple": True, "typeClass": "primitive", "value": [""]},
            'timeMethod': {"typeName": "timeMethod", "multiple": False, "typeClass": "primitive", "value": ""},
            'dataCollector': {"typeName": "dataCollector", "multiple": False, "typeClass": "primitive", "value": ""},
            'collectorTraining': {"typeName": "collectorTraining", "multiple": False, "typeClass": "primitive", "value": ""},
            'frequencyOfDataCollection': {"typeName": "frequencyOfDataCollection", "multiple": False, "typeClass": "primitive", "value": ""},
            'samplingProcedure': {"typeName": "samplingProcedure", "multiple": False, "typeClass": "primitive", "value": ""},
            'targetSampleSize': {"typeName": "targetSampleSize", "multiple": False, "typeClass": "compound", "value": ['']},
            'deviationsFromSampleDesign': {"typeName": "deviationsFromSampleDesign", "multiple": False, "typeClass": "primitive", "value": ""},
            'collectionMode': {"typeName": "collectionMode", "multiple": True, "typeClass": "primitive", "value": [""]},
            'researchInstrument': {"typeName": "researchInstrument", "multiple": False, "typeClass": "primitive", "value": ""},
            'dataCollectionSituation': {"typeName": "dataCollectionSituation", "multiple": False, "typeClass": "primitive", "value": ""},
            'actionsToMinimizeLoss': {"typeName": "actionsToMinimizeLoss", "multiple": False, "typeClass": "primitive", "value": ""},
            'controlOperations': {"typeName": "controlOperations", "multiple": False, "typeClass": "primitive", "value": ""},
            'weighting': {"typeName": "weighting", "multiple": False, "typeClass": "primitive", "value": ""},
            'cleaningOperations': {"typeName": "cleaningOperations", "multiple": False, "typeClass": "primitive", "value": ""},
            'datasetLevelErrorNotes': {"typeName": "datasetLevelErrorNotes", "multiple": False, "typeClass": "primitive", "value": ""},
            'responseRate': {"typeName": "responseRate", "multiple": False, "typeClass": "primitive", "value": ""},
            'samplingErrorEstimates': {"typeName": "samplingErrorEstimates", "multiple": False, "typeClass": "primitive", "value": ""},
            'otherDataAppraisal': {"typeName": "otherDataAppraisal", "multiple": False, "typeClass": "primitive", "value": ""},
            'socialScienceNotes': {"typeName": "socialScienceNotes", "multiple": False, "typeClass": "compound", "value": ['']}
        }
        block_name = 'socialscience'


    # Geospatial metadata block configuration
    elif 'geospatial' in headers:
        field_directory = {
            'geographicCoverage': {"typeName": "geographicCoverage", "multiple": True, "typeClass": "compound", "value": [""]},
            'geographicUnit': {"typeName": "geographicUnit", "multiple": True, "typeClass": "primitive", "value": [""]},
            'geographicBoundingBox': {"typeName": "geographicBoundingBox", "multiple": True, "typeClass": "compound", "value": []}
        }
        block_name = 'geospatial'


    # Astrophysics metadata block configuration
    elif 'astrophysics' in headers:
        field_directory = {
            'astroType': {"typeName": "astroType", "multiple": True, "typeClass": "controlledVocabulary", "value": [""]},
            'astroFacility': {"typeName": "astroFacility", "multiple": True, "typeClass": "primitive", "value": [""]},
            'astroInstrument': {"typeName": "astroInstrument", "multiple": True, "typeClass": "primitive", "value": [""]},
            'astroObject': {"typeName": "astroObject", "multiple": True, "typeClass": "primitive", "value": [""]},
            'resolution.Spatial': {"typeName": "resolution.Spatial", "multiple": False, "typeClass": "primitive", "value": ""},
            'resolution.Spectral': {"typeName": "resolution.Spectral", "multiple": False, "typeClass": "primitive", "value": ""},
            'resolution.Temporal': {"typeName": "resolution.Temporal", "multiple": False, "typeClass": "primitive", "value": ""},
            'coverage.Spectral.Bandpass': { "typeName": "coverage.Spectral.Bandpass", "multiple": True, "typeClass": "primitive", "value": [""]},
            'coverage.Spectral.CentralWavelength': {"typeName": "coverage.Spectral.CentralWavelength", "multiple": True, "typeClass": "primitive", "value": [""]},
            'coverage.Spectral.Wavelength': {"typeName": "coverage.Spectral.Wavelength", "multiple": True, "typeClass": "compound", "value": [""]},
            'coverage.Temporal': {"typeName": "coverage.Temporal", "multiple": True, "typeClass": "compound", "value": []},
            'coverage.Spatial': {"typeName": "coverage.Spatial", "multiple": True, "typeClass": "primitive", "value": [""]},
            'coverage.Depth': {"typeName": "coverage.Depth", "multiple": False, "typeClass": "primitive", "value": ""},
            'coverage.ObjectDensity': {"typeName": "coverage.ObjectDensity", "multiple": False, "typeClass": "primitive", "value": ""},
            'coverage.ObjectCount': {"typeName": "coverage.ObjectCount", "multiple": False, "typeClass": "primitive", "value": ""},
            'coverage.SkyFraction': {"typeName": "coverage.SkyFraction", "multiple": False, "typeClass": "primitive", "value": ""},
            'coverage.Polarization': {"typeName": "coverage.Polarization", "multiple": False, "typeClass": "primitive", "value": ""},
            'redshiftType': {"typeName": "redshiftType", "multiple": False, "typeClass": "primitive", "value": ""},
            'resolution.Redshift': {"typeName": "resolution.Redshift", "multiple": False, "typeClass": "primitive", "value": ""},
            'coverage.RedshiftValue': {"typeName": "coverage.RedshiftValue", "multiple": True, "typeClass": "compound", "value": [""]}
        }
        block_name = 'astrophysics'


    # Biomedical metadata block configuration
    elif 'biomedical' in headers:
        field_directory = {
            'studyDesignType': {"typeName": "studyDesignType", "multiple": True, "typeClass": "controlledVocabulary", "value": [""]},
            'studyOtherDesignType': {"typeName": "studyOtherDesignType", "multiple": True, "typeClass": "primitive", "value": [""]},
            'studyFactorType': {"typeName": "studyFactorType", "multiple": True, "typeClass": "controlledVocabulary", "value": [""]},
            'studyOtherFactorType': {"typeName": "studyOtherFactorType", "multiple": True, "typeClass": "primitive", "value": [""]},
            'studyAssayOrganism': {"typeName": "studyAssayOrganism", "multiple": True, "typeClass": "controlledVocabulary", "value": [""]},
            'studyAssayOtherOrganism': {"typeName": "studyAssayOtherOrganism", "multiple": True, "typeClass": "primitive", "value": [""]},
            'studyAssayMeasurementType': {"typeName": "studyAssayMeasurementType", "multiple": True, "typeClass": "controlledVocabulary", "value": [""]},
            'studyAssayOtherMeasurmentType': {"typeName": "studyAssayOtherMeasurmentType", "multiple": True, "typeClass": "primitive", "value": [""]},
            'studyAssayTechnologyType': {"typeName": "studyAssayTechnologyType", "multiple": True, "typeClass": "controlledVocabulary", "value": [""]},
            'studyAssayOtherTechnologyType': {"typeName": "studyAssayOtherTechnologyType", "multiple": True, "typeClass": "primitive","value": [""]},
            'studyAssayPlatform': {"typeName": "studyAssayPlatform", "multiple": True, "typeClass": "controlledVocabulary", "value": [""]},
            'studyAssayOtherPlatform': {"typeName": "studyAssayOtherPlatform", "multiple": True, "typeClass": "primitive", "value": [""]},
            'studyAssayCellType': {"typeName": "studyAssayCellType", "multiple": True, "typeClass": "primitive", "value": [""]}
        }
        block_name = 'biomedical'


    # Journal metadata block configuration
    elif 'journal' in headers:
        field_directory = {
            'journalVolumeIssue': {"typeName": "journalVolumeIssue", "multiple": True, "typeClass": "compound", "value": [""]},
            'journalArticleType': {"typeName": "journalArticleType", "multiple": False, "typeClass": "controlledVocabulary", "value": ""}
        }
        block_name = 'journal'


    # Computational Work Flow metadata block configuration
    elif 'computationalworkflow' in headers:
        field_directory = {
            'workflowType': {"typeName": "workflowType", "multiple": True, "typeClass": "controlledVocabulary", "value": [""]},
            'workflowCodeRepository': {"typeName": "workflowCodeRepository", "multiple": True, "typeClass": "primitive", "value": [""]},
            'workflowDocumentation': {"typeName": "workflowDocumentation", "multiple": True, "typeClass": "primitive", "value": [""]},
        }
        block_name = 'computationalworkflow'


    # 3D Objects metadata block configuration
    elif '3dobjects' in headers:
        field_directory = {
            '3d3DTechnique': {"typeName": "3d3DTechnique", "multiple": False, "typeClass": "controlledVocabulary", "value": ""},
            '3dEquipment': {"typeName": "3dEquipment", "multiple": False, "typeClass": "primitive", "value": ""},
            '3dLightingSetup': {"typeName": "3dLightingSetup", "multiple": False, "typeClass": "controlledVocabulary", "value": ""},
            '3dMasterFilePolygonCount': {"typeName": "3dMasterFilePolygonCount", "multiple": False, "typeClass": "primitive", "value": ""},
            '3dExportedFilePolygonCount': {"typeName": "3dExportedFilePolygonCount", "multiple": True, "typeClass": "primitive", "value": [""]},
            '3dExportedFileFormat': {"typeName": "3dExportedFileFormat", "multiple": False, "typeClass": "controlledVocabulary", "value": ""},
            '3dAltText': { "typeName": "3dAltText", "multiple": False, "typeClass": "primitive", "value": ""},
            '3dMaterialComposition': {"typeName": "3dMaterialComposition", "multiple": True, "typeClass": "primitive", "value": [""]},
            '3dObjectDimensions': {'typeName': '3dObjectDimensions', "multiple": False, "typeClass": "compound", "value": [""]},
            '3dHandling': {"typeName": "3dHandling", "multiple": False, "typeClass": "primitive", "value": ""}
        }
        block_name = '3dobjects'


    # Build master lists of field types
    master_lists = []
    primitive_fields = []
    compound_fields = []
    controlled_vocab_fields = []

    for field_name, field_def in field_directory.items():
        if field_def['typeClass'] == 'primitive':
            primitive_fields.append(field_name)
        elif field_def['typeClass'] == 'compound':
            compound_fields.append(field_name)
        elif field_def['typeClass'] == 'controlledVocabulary':
            #controlled_vocab_fields.append(field_name)
            primitive_fields.append(field_name) # CONTROLLED VOCAB ADDED TO PRIMITIVE LIST - WILL BE UPDATED IN THE FUTURE

    master_lists = [primitive_fields, compound_fields, controlled_vocab_fields]
    print(master_lists)
    print(master_lists[1])

    return [field_directory, block_name, master_lists]





def metadatablock_generator(block):

    if block == 'socialscience':
        block_directory = {"socialscience": {"displayName": "Social Science and Humanities Metadata", "name": "socialscience", "fields": []}}
    elif block == 'geospatial':
        block_directory = {"geospatial": {"displayName": "Geospatial Metadata", "name": "geospatial", "fields": []}}
    elif block == 'astrophysics':
        block_directory = {"astrophysics": {"displayName": "Astronomy and Astrophysics Metadata", "name": "astrophysics", "fields": []}}
    elif block == 'biomedical':
        block_directory = {"biomedical": {"displayName": "Life Sciences Metadata", "name": "biomedical", "fields": []}}
    elif block == 'journal':
        block_directory = {"journal": {"displayName": "Journal Metadata", "name": "journal", "fields": []}}
    elif block == 'computationalworkflow':
        block_directory = {"computationalworkflow": {"displayName": "Computational Workflow Metadata", "name": "computationalworkflow", "fields": []}}
    elif block == '3dobjects':
        block_directory = {"3dobjects": {"displayName": "3D Objects Metadata", "name": "3dobjects", "fields": []}}

    return block_directory





def API_push(field, doi):
    """
    Push metadata updates to the Dataverse API.

    Sends a PUT request to update dataset metadata with the provided field changes.

    Args:
        field (dict): Field data to update
        doi (str): Dataset DOI
    """
    print(json.dumps(field))
    url = f'{url_base_origin}/api/datasets/:persistentId/editMetadata?persistentId={doi}&replace=true'
    print(url)

    resp = requests.put(url, data=json.dumps(field), headers=headers_origin)
    print(resp.json())
    print(resp.status_code)
    print()





def publish_dataset(doi):
    """
    Publish dataset with a minor version increment.

    Optional function to automatically publish datasets after metadata updates.

    Args:
        doi (str): Dataset DOI

    Returns:
        int: HTTP status code from the publish operation
    """
    resp = api_origin.publish_dataset(doi, "minor")
    return resp.status_code



# ============================================================================
# SCRIPT EXECUTION
# ============================================================================

if __name__ == "__main__":
    file_loader()


