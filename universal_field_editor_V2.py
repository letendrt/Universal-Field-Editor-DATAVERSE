# -*- coding: utf-8 -*-
"""Universal_Field_Editing_Script.ipynb

Automatically generated by Colab.







# UNIVERSAL EDITING SCRIPT
"""

# Library Import

#!pip install pyDataverse

from datetime import datetime
import sys
import time
import csv
import json
import pandas as pd


# Importing libraries for dataverse api
# Documentation found here: https://pydataverse.readthedocs.io/en/latest/
import requests
import pyDataverse.utils as utils
from pyDataverse.api import NativeApi, DataAccessApi

# API and URL configuration

# create a list of directories for each CSV sheet (citation, social sciences, etc.) If only one sheet, still put in list format
file_directory = ['DIRECTORY/FOR/Citation Metadata.csv', 'DIRECTORY/FOR/Social Science and Humanities.csv']

api_token_origin = "API KEY HERE"                                                  # Enter API key as string
url_base_origin = 'BASE URL HERE'                                                  # Input base origin url as string (e.g., https://demo.borealisdata.ca)

headers_origin = {'X-Dataverse-key': api_token_origin}                             # Create dictionary and insert API token as the value
api_origin = NativeApi(url_base_origin, api_token_origin)                          # API call using the pyDataverse
data_api_origin = DataAccessApi(url_base_origin, api_token_origin)

# This function checks if a dataset is locked (for one reason or another; it could be due to .tab file ingestion, for instance)
# More documentation on what this entails available here: https://guides.dataverse.org/en/6.2/api/native-api.html#dataset-locks







def check_lock(dataset_id):

    time_start = datetime.now()                                                     # Set up internal timer
    print("Start check_lock")                                                       # Print start message

    try:                                                                            # Checking if url is accessible.
        url = f"{url_base_origin}/api/datasets/{dataset_id}/locks"                  # Set up URL for access
        lock = requests.get(url, headers_origin)                                    # Set up lock status

        if lock.status_code == 503:                                                 # If URL is unavailable
            print("503 - Server is unavailable")                                    # Print status
            sys.exit()                                                              # stop running the function

        a = 0                                                                       # Starting attempt tracker
        while len(lock.json()['data']) > 0:                                         # If the tracker is above 0 (meaning dataset is locked)
            print(f"Lock {str(a)} times {dataset_id} {lock.json()}")                # API returns jason file of all locked datasets
            print(lock.json())                                                      # Print lock information
            time.sleep(10)                                                          # Start a 10 second timer
            a += 1                                                                  # Update attempt tracker

            lock = requests.get(url, headers_origin)                                # Check lock status
            if lock.status_code == 503:                                             # If URL is unavailable
                print("503 - Server is unavailable")                                # Print status
                sys.exit()                                                          # Stop running the function

            if lock.status_code != 200:                                                                 # If URL status code is not 200 (not successful)
                print(f"check_lock func: lock status {str(lock.status_code)} for {dataset_id}")         # Print lock information
                return False                                                                            # Return False

    except Exception as e:                                                          # This exception parameter prevents unwanted crashes or errors.
        print(f"check_lock. Error {str(e)}, dataset {dataset_id} ")                 # Print lock information
        return False                                                                # Return False

    time_end = datetime.now()                                                       # Provide date and time
    t = (time_end - time_start)                                                     # Give total time taken to ingest dataset
    print(f"Dataset {str(dataset_id)} was locked {str(t.total_seconds())} sec")     # Provide dataset information

    return True                                                                     # Return True







# This function parses through the metadata blocks and the CSV file row
# to retrieve to-be updated metadata. It formats the new metadata field value
# by calling other functions (see below). It then pushes the updated fields
# to the API

# It takes 6 arguments:
# - latest_version: most recent version of the metadatablock (pulled form borealis);
# - row: the row of the CSV file;
# - doi: the doi of the dataset being modified;
# - header: list of all to-be parsed field column names;
# - directory: dictionary of all the main-level citation metadata fields (defined in file_loader());
# - master_list: list of lists defined in file_loader() ----
#           master_list[0] = primitive field typeNames, master_list[1] = compound field typeNames

def update_metadata(latest_version, row, doi, header, directory, master_list, block):

    updated_citation = {}                                                            # Creating empty dictionary for indexing (I am aware that this is json formatting as well as dictionary)
    updated_citation['fields'] = []                                                  # Creating new dictionary lock for keywords
    updated_series = {}                                                              # Creating empty dictionary for indexing (I am aware that this is json formatting as well as dictionary)
    metadataBlocks = latest_version['metadataBlocks']                                # Retrieving metadata for the dataset and assining it to the variable metadataBlocks
    fields = metadataBlocks[block]['fields']                                         # Retrieving metadata fields from the extracted metadata blocks

    print('%%%%%%%%%%%%%%%%%%%%%%%%%%%%')
    print(fields)                                                                    # Prints all fields from the existing citation metadata blocks
    print('%%%%%%%%%%%%%%%%%%%%%%%%%%%%')

    existing_fields = [i for i in fields]                                            # Lists all the existing fields from the current record
    existing_field_typeNames = [i['typeName'] for i in fields]                       # Lists the name of all the existing fields from the current record
    print(existing_fields)
    print(existing_field_typeNames)


    root_counter = 0                                                                 # Setting up counter used in list parsing
    for change_area in header:                                                       # For all items in metadata fields
        assess = change_area.split(":")                                              # Split the header at the colon (item on the right of the column is the field typeName)
        print(assess[0])                                                             # Print the field typeName

        if assess[0] not in existing_field_typeNames and assess[0] != 'doi':         # If the existing record does not hold the current field typeName (and is not a DOI)
            print(f"{assess[0]} -- NOT IN EXISTING RECORD")                          # Indicate that the metadata field is not in the current record

            if row[change_area] != '':                                               # If the CSV cell associated to this field is not empty
                print(f'RECORD TO ADD: {row[change_area]}')                          # Print the content of the cell (the to-be added value)

                field = directory[assess[0]]                                         # Assigning respective directory value to the "field" variable (see file_loader() function for directory dictionary)

                if assess[0] in master_list[0]:                                      # If the typeName is in master_list[0] (meaning it is primitive)
                    output = primitive_formatter(change_area, row, field)            # Assign function output to the "output" variable
                    print(f'OUTPUT: {output}')

                    if output == '':                                                 # If the output is an empty string (meaning there is nothing to update the record with - shouldn't really happen here)
                        print()                                                      # This is for log legibility
                        root_counter += 1
                        continue                                                     # Continue to next loop item

                    else:                                                            # Otherwise
                        API_push(field, doi)                                         # Push metadata changes to API

                if assess[0] in master_list[1]:                                      # If the typeName is in master_list[1] (meaning it is primitive)
                    output = compound_formatter(change_area, row)                    # Assign function output to the "output" variable

                    if output == False:                                              # If output is False (potential output from the function above)
                        print()                                                      # This is for log legibility
                        root_counter += 1
                        continue                                                     # Continue to the next loop item

                    else:                                                            # If output is not False (meaning it is an actual value)
                        field['value'] = output                                      # Assign the output to the field value
                        print(field)                                                 # Print the field
                        API_push(field, doi)                                         # Push to API

            else:                                                                    # If the CSV cell for the field is empty
                print('-- NO RECORD TO ADD --')                                      # Enter log entry that no update is required
                print()                                                              # This is for log legibility
                continue                                                             # Continue to next loop item

        elif assess[0] in existing_field_typeNames:                                  # If the column fieled typeName is not in the existing record

            fields = existing_fields[root_counter]                                   # Assign the existing indexed field record to the variable "fields"
            print(f'Change_area = {change_area}')                                    # Print which area is to be changed
            print(f'Fields = {fields['typeName']}')                                  # Print the related field

            if fields['typeName'] == change_area and fields['typeName'] in master_list[0]:             # If the field typeName is in the existing record and is in the primitive category

                field = existing_fields[root_counter]                                          # Assign the existing record field to the variable 'field' (not sure why I didn't reuse 'fields')
                output = primitive_formatter(change_area, row, field)                          # Assign returned function value to the variable "output"
                print(f'OUTPUT: {output}')                                                     # Print the function output

                if output == '':                                                     # If the output is an empty string
                    print()                                                          # Gap for log legibility
                    root_counter += 1
                    continue                                                         # Continue to the next loop item

                else:                                                                # Otherwise
                    API_push(field, doi)                                             # Push to API


            if fields['typeName'] in change_area and fields['typeName'] in master_list[1]:           # If the field typeName is in the existing record and is in the compound category
                field_format = compound_formatter(change_area, row)                                  # Assign function output to the field_format variable
                field = existing_fields[root_counter]                                                # Assign the existing record field to the variable 'field' (not sure why I didn't reuse 'fields')


                if field_format == False:                                            # If the function output is False
                    print()                                                          # Gap for legibility
                    root_counter += 1
                    continue                                                         # Continue to next loop item

                else:                                                                # Otherwise
                    if fields['multiple'] == True:
                        field['value'] = field_format
                    else:
                      field['value'] = field_format[0]

                    API_push(field, doi)                                             # Push to API

            root_counter += 1                                                        # Increase root_counter for next item index retrieval

    #return True                                                                      # Optional return fetaure, only for auto publish






# This function formats the values of primitive metadata fields if an only if
# they are imbedded in a compound field (meaning that it is a first-level field)

# Takes 3 arguments:
# - change_area: the name of the field whose value is being updated
# - row: CSV row in which the new values are held
# - field: JSON/dictionary format that will be modified and returned

def primitive_formatter(change_area, row, field):

    unit_to_replace = field['value']                                               # Pull from the existing record the to-be replaced value.
    print(f"Unit to replace = {unit_to_replace}")                                  # Print fetched content

    if '+' in row[change_area]:                                                    # If there is a + in the cell holding the new values (meaning there are multiple new values)
        new_unit = row[change_area].split('+')                                     # Create a new list of items - split at the +
        print(f'New unit (plural): {new_unit}')                                    # Printing new value list

        field_val = record_check(new_unit, field)                                  # assign record_check function output to the variable 'field_formatter'
        return field_val                                                           # return field_val

    else:                                                                          # If there is not + sign in the value cell (meaning only one new value)
        if field['multiple'] == True:                                              # If the field must hold an array
            new_unit = [row[change_area]]                                          # list the new_value
            print(f'New unit (singular): {new_unit}')                              # Printing new value list

            field_val = record_check(new_unit, field)                              # assign record_check function output to the variable 'field_formatter'
            return field_val                                                       # return field_val

        else:
            new_unit = row[change_area]                                            # Assign the new value to the variable 'new_unit'
            print(f'New unit (singular): {new_unit}')                              # Print new value

            field_val = record_check(new_unit, field)                              # assign record_check function output to the variable 'field_formatter'
            return field_val                                                       # return field_val







# This function was made to avoid code redundancy in the above function.

# Takes 2 arguments:
# - new_unit: the new value used to modify the existing record;
# - field: the existing record in which the value is added.

def record_check(new_unit, field):

    if new_unit == [''] or new_unit == '':                     # If this list is empty (or is an empty string)
        print('NOT UPDATED IN THE RECORD')                     # Log that no update has been followed up upon
        record = ''                                            # Assign empty string value to the variable 'record'
        print()                                                # Gap for log legibility
        return record                                          # return an empty string

    else:
        field['value'] = new_unit                              # Inserting new entry in previous value field
        return field                                           # return new field value







# This function is used to format metadata fields that are compound in nature
# (meaning that they hold primitive fields as values)

# Takes 2 arguments:
# - header: name of the CSV column currently being parsed through by update_metadata_name()
# - row: the CSV file row for the dataset

def compound_formatter(header, row):

    first_list = header.split(":")                                             # Split the header at the colon

    if ';' in first_list[1]:                                                   # If there are semicolons (meaning there are many primitive fields for the compound field)
        compound_list = [i.strip() for i in first_list[1].split(';')]          # Create a list of the primitive fields from the header string (spliting them at the ; )

    else:                                                                      # Otherwise
        compound_list = [first_list[1].strip()]                                # List of the primitive field in the compound field


    citation_dictionary = {}                                                   # Create empty dictionary
    citation_dictionary[f'{first_list[0]}'] = compound_list                    # assign the compound field name as the key, and the list of primitives as the value
    print(f"Citation dictionary = {citation_dictionary}")                      # Print the dictioanry for the log
    print(f'Compound list = {compound_list}')                                  # Print list for the log


    if row[header] != '':                                                      # If the value cell is not empty (populated)

        if '+' in row[header]:                                                 # If the value cell holds '+' (indicates recursion)
            val_list = row[header].split('+')                                  # Create a list of the call values by spliting the string at the '+'s
            value_list = []                                                    # Create an empty list
            for i in val_list:                                                 # For values in the recursion list
                value = i.split(';')                                           # Split list values at the ; (indicates different primitive fields)
                value_list.append(value)                                       # Add the values to the empty string

        else:                                                                  # Otherwise (no + signs)
            value_list = row[header].split(';')                                # Split the list at the semicolons (meaning different primitive fields)

        print(f"Value list = {value_list}")                                    # Print the list of values

    else:                                                                      # If the value cell is an empty string
        value_list = []                                                        # Create an empty list

    counter = 0                                                                # Start a parsing counter
    list_of_list = []                                                          # Create an empty list

    for types in citation_dictionary[f'{first_list[0]}']:                      # For the field metadata types
        populated_dictionary = {}                                              # Create an empty dictionary

        if counter >= len(value_list):                                         # If the counter is past the max index
            break                                                              # Break the loop

        if value_list == [''] or value_list == []:                             # If the cell list is empty
          counter += 1                                                         # Increase parsing counter
          continue                                                             # continue to next loop item

        else:                                                                  # Otherwise
            list_of_list = []                                                  # cretae an empty list
            counter_2 = 0                                                      # Start a fresh counter

            for items in value_list:                                           # for items in our list of values
                val_setup = {}                                                 # Create a new empty dictionary

                if type(items) is list:                                        # If the item being parsed is in itself a list (meaning there are different primitive fields)
                    count = 0                                                  # Start fresh counter

                    for i in items:                                                         # For items within the respective list
                        i = i.strip()                                                       # Strip the string of empty spaces
                        primitive_dictionary = {}                                           # Create empty dictionary
                        primitive_dictionary['typeName'] = compound_list[count].strip()     # Setup typeName by fetching the respective primitive metadata fielf typeName
                        primitive_dictionary['multiple'] = False                            # Set to False
                        primitive_dictionary['typeClass'] = "primitive"                     # Identify as primitive
                        primitive_dictionary['value'] = i                                   # Assign the item to the new dictionary value field

                        val_setup[compound_list[count].strip()] = primitive_dictionary      # Index the metadata entry dictionary in the value dictionary
                        count += 1                                                          # Update count

                else:                                                                                   # Otherwise (no list, meaning no for looping)
                    items = items.strip()                                                               # Strip the string of empty spaces
                    primitive_dictionary = {}                                                           # Create empty dictionary
                    primitive_dictionary['typeName'] = compound_list[counter_2].strip()                 # Setup typeName by fetching the respective primitive metadata fielf typeName
                    primitive_dictionary['multiple'] = False                                            # Set to False
                    primitive_dictionary['typeClass'] = "primitive"                                     # Identify as primitive
                    primitive_dictionary['value'] = items                                               # Assign the item to the new dictionary value field

                    populated_dictionary[compound_list[counter_2].strip()] = primitive_dictionary       # Index the metadata entry dictionary in a higher level dictionary
                    counter_2 += 1                                                                      # Update indexed counter

                if val_setup != {}:                                            # If the dictionary is populated (not empty)
                  list_of_list.append(val_setup)                               # add the dictionary in the list of list (list of dictionary)

        if val_setup == {}:                                                    # If the dictionary is empty
            list_of_list.append(populated_dictionary)                          # Add the populated dictionary to the list of list

        counter += 1                                                           # Updated indexing counter

    if list_of_list == []:                                                     # if the list of list is empty
        print("NOT UPDATED IN THE RECORD")                                     # Log that there was no update
        return False                                                           # return false

    else:                                                                      # Otherwise
        return list_of_list                                                    # return the list of list







# File load and selection. This is the first function to run.
# All others a called form this one

def file_loader():

    for csv_directory in file_directory:

        df = pd.read_csv(csv_directory)                                           # Read csv file and create dataframe
        header = list(df.columns)                                                  # Create list of headers
        print(header)                                                              # Print the list of column headers

        info = xml_selecter(header)

        directory = info[0]
        block = info[1]
        master_list = info[2]


        with (open(csv_directory, newline='', encoding='utf-8-sig') as csvfile):   # Read csv file
            reader = csv.DictReader(csvfile)                                       # setup reader

            for row in reader:                                                     # For each row
                print(row)                                                         # Print the row
                doi = row[header[0]]                                               # assign the doi to the variable 'doi'

                if 'https://doi.org/' in doi:                                      # If the doi is setup in link format
                  doi = doi.replace('https://doi.org/', 'doi:')                    # Replace it with standard format
                print(doi)                                                         # Print formatted doi

                resp = api_origin.get_dataset(doi, version="2.0")                  # Fetch dataset
                print(resp.json())                                                 # Print response code

                if resp.status_code == 200:                                             # If API call is successful (marker 200)
                    id = resp.json()['data']['id']                                      # old dataset ID is assigned to variable id
                    latest_version = resp.json()['data']['latestVersion']               # Latest old dataset version is set to variable list "latest_version"
                    print(latest_version)


                pub_stat = update_metadata(latest_version, row, doi, header, directory, master_list, block)       # Assign fuinction output to the variable 'pub_stat' - optional, only if for auto-publish


                #if pub_stat == True:                                              # If pub_stab is returned as True
                    #publish_dataset(doi)                                          # Run API function to publish dataset







def xml_selecter(header):

    if 'MARKER 1' in header:

        # Note: this will be edited to pull from the dataverse .TSV files eventually (https://github.com/IQSS/dataverse/tree/develop/scripts/api/data/metadatablocks)
        directory = {'title': {"typeName": "title", "multiple": False, "typeClass": "primitive", "value": ""},
                  'subtitle': {"typeName": "subtitle", "multiple": False, "typeClass": "primitive", "value": ""},
                  'alternativeTitle': {"typeName": "alternativeTitle", "multiple": True, "typeClass": "primitive", "value": [""]},
                  'otherId': {"typeName": "otherId", "multiple": True, "typeClass": "compound", "value": [""]},
                  'author': {"typeName": "author", "multiple": True, "typeClass": "compound", "value": [""]},
                  'datasetContact': {"typeName": "datasetContact", "multiple": True, "typeClass": "compound", "value": [""]},
                  'dsDescription': {"typeName": "dsDescription", "multiple": True, "typeClass": "compound", "value": [""]},
                  'subject': {"typeName": "subject", "multiple": True, "typeClass": "controlledVocabulary", "value": [""]},
                  'keyword': {"typeName": "keyword", "multiple": True, "typeClass": "compound", "value": [""]},
                  'topicClassification': {"typeName": "topicClassification", "multiple": True, "typeClass": "compound", "value": [""]},
                  'publication': {"typeName": "publication", "multiple": True, "typeClass": "compound", "value": [""]},
                  'notesText': {"typeName": "notesText", "multiple": False, "typeClass": "primitive", "value": ""},
                  'language': {"typeName": "language", "multiple": True, "typeClass": "controlledVocabulary", "value": [""]},
                  'producer': {"typeName": "producer", "multiple": True, "typeClass": "compound", "value": [""]},
                  'productionDate': {"typeName": "productionDate", "multiple": False, "typeClass": "primitive", "value": ""},
                  'productionPlace': {"typeName": "productionPlace", "multiple": True, "typeClass": "primitive", "value": [""]},
                  'contributor': {"typeName": "contributor", "multiple": True, "typeClass": "compound", "value": [""]},
                  'grantNumber': {"typeName": "grantNumber", "multiple": True, "typeClass": "compound", "value": [""]},
                  'distributor': {"typeName": "distributor", "multiple": True, "typeClass": "compound", "value": [""]},
                  'distributionDate': {"typeName": "distributionDate", "multiple": False, "typeClass": "primitive", "value": ""},
                  'depositor': {"typeName": "depositor", "multiple": False, "typeClass": "primitive", "value": ""},
                  'dateOfDeposit': {"typeName": "dateOfDeposit", "multiple": False, "typeClass": "primitive", "value": ""},
                  'timePeriodCovered': {"typeName": "timePeriodCovered", "multiple": True, "typeClass": "compound", "value": [""]},
                  'dateOfCollection': {"typeName": "dateOfCollection", "multiple": True, "typeClass": "compound", "value": [""]},
                  'kindOfData': {"typeName": "kindOfData", "multiple": True, "typeClass": "primitive", "value": [""]},
                  'series': {"typeName": "series", "multiple": True, "typeClass": "compound", "value": [""]},
                  'software': {"typeName": "software", "multiple": True, "typeClass": "compound", "value": [""]},
                  'relatedMaterial': {"typeName": "relatedMaterial", "multiple": True, "typeClass": "primitive", "value": [""]},
                  'relatedDatasets': {"typeName": "relatedDatasets", "multiple": True, "typeClass": "primitive", "value": [""]},
                  'otherReferences': {"typeName": "otherReferences", "multiple": True, "typeClass": "primitive", "value": [""]},
                  'dataSources': {"typeName": "dataSources", "multiple": True, "typeClass": "primitive", "value": [""]},
                  'originOfSources': {"typeName": "originOfSources", "multiple": False, "typeClass": "primitive", "value": ""},
                  'characteristicOfSources': {"typeName": "characteristicOfSources", "multiple": False, "typeClass": "primitive", "value": ""},
                  'accessToSources': {"typeName": "accessToSources", "multiple": False, "typeClass": "primitive", "value": ""}
                  }

        block = 'citation'

   
    if 'MARKER 2' in header:

        directory = {'unitOfAnalysis': {"typeName": "unitOfAnalysis", "multiple": True, "typeClass": "primitive", "value": [""]},
                    'universe': {"typeName": "universe", "multiple": True, "typeClass": "primitive", "value": [""]},
                    'timeMethod': {"typeName": "timeMethod", "multiple": False, "typeClass": "primitive", "value": ""},
                    'dataCollector': {"typeName": "dataCollector", "multiple": False, "typeClass": "primitive", "value": ""},
                    'collectorTraining': {"typeName": "collectorTraining", "multiple": False, "typeClass": "primitive", "value": ""},
                    'frequencyOfDataCollection': {"typeName": "frequencyOfDataCollection", "multiple": False, "typeClass": "primitive", "value": ""},
                    'samplingProcedure' : {"typeName": "samplingProcedure", "multiple": False, "typeClass": "primitive", "value": ""},
                    'targetSampleSize': {"typeName": "targetSampleSize", "multiple": False, "typeClass": "compound", "value": ['']},
                    'deviationsFromSampleDesign': {"typeName": "deviationsFromSampleDesign", "multiple": False, "typeClass": "primitive", "value": ""},
                    'collectionMode': {"typeName": "collectionMode", "multiple": True, "typeClass": "primitive", "value":[""]},
                    'researchInstrument': {"typeName": "researchInstrument", "multiple": False, "typeClass": "primitive", "value": ""},
                    'dataCollectionSituation': {"typeName": "dataCollectionSituation", "multiple": False, "typeClass": "primitive", "value": ""},
                    'actionsToMinimizeLoss': {"typeName": "actionsToMinimizeLoss", "multiple": False, "typeClass": "primitive", "value": ""},
                    'controlOperations': {"typeName": "controlOperations", "multiple": False, "typeClass": "primitive", "value": ""},
                    'weighting': {"typeName": "weighting", "multiple": False, "typeClass": "primitive", "value": ""},
                    'cleaningOperations': {"typeName": "cleaningOperations", "multiple": False, "typeClass": "primitive", "value": ""},
                    'datasetLevelErrorNotes': {"typeName": "datasetLevelErrorNotes", "multiple": False, "typeClass": "primitive", "value": ""},
                    'responseRate': {"typeName": "responseRate", "multiple": False, "typeClass": "primitive", "value": ""},
                    'samplingErrorEstimates': {"typeName": "samplingErrorEstimates", "multiple": False, "typeClass": "primitive", "value": ""},
                    'otherDataAppraisal': {"typeName": "otherDataAppraisal", "multiple": False, "typeClass": "primitive", "value": ""},
                    'socialScienceNotes': {"typeName": "socialScienceNotes", "multiple": False, "typeClass": "compound", "value": ['']}
                    }

        block = 'socialscience'

    master_list = []                                                           # Creating a list of list
    primitive_list = []                                                        # Creating the list of primitive fields
    compound_list = []                                                         # Creating the list of compound fields
    controlled_vocab_list = []                                                 # Creating the list of controlled vocabs (not currently in use)

    for k, v in directory.items():                                             # For keys and values in the above directory dictionary
        if v['typeClass'] == 'primitive':                                      # If the type class is primitve
            primitive_list.append(k)                                           # Add to primitive list
        elif v['typeClass'] == 'compound':                                     # If the type class is coumpound
            compound_list.append(k)                                            # Add to compound list
        elif v['typeClass'] == 'controlledVocabulary':                         # If type class is controlled vocab
            primitive_list.append(k)                                           # Add to primitive (this will eventually be changed)
            #controlled_vocab_list.append(k)

    master_list.append(primitive_list)                                         # Add list to master_list
    master_list.append(compound_list)                                          # Add list to master_list
    master_list.append(controlled_vocab_list)                                  # Add list to master_list

    print(master_list)

    return [directory, block, master_list]








# Push to API

def API_push(field, doi):

    print(json.dumps(field))                                                                                # Print new metadata field (converts python dictionary to json format)
    url = f'{url_base_origin}/api/datasets/:persistentId/editMetadata?persistentId={doi}&replace=true'      # Update URL variable for API call
    print(url)                                                                                              # Print URL

    resp = requests.put(url, data=json.dumps(field), headers=headers_origin)                                # Upload the updated metadata fields after converting the dictionaries to json format
    print(resp.json())                                                                                      # Print metadata block in json format (?)
    print(resp.status_code)
    print()








# Publish Dataset function - optional

def publish_dataset(doi):
    resp = api_origin.publish_dataset(doi, "minor")                            # Publish type set to minor change - case by case basis
    return resp.status_code

file_loader()







