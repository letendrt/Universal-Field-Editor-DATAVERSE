# -*- coding: utf-8 -*-
"""
Universal Field Editor Script for Dataverse

A script for bulk editing metadata fields in Dataverse datasets using CSV files.
This script connects to a Dataverse instance via its API and updates dataset metadata
based on values specified in CSV files.

Automatically generated by Colab and enhanced with proper documentation.
"""

# Standard library imports
from datetime import datetime
import sys
import time
import csv
import json

# Third-party imports
import pandas as pd
import requests

# Dataverse API imports
# Documentation: https://pydataverse.readthedocs.io/en/latest/
import pyDataverse.utils as utils
from pyDataverse.api import NativeApi, DataAccessApi

# ============================================================================
# CONFIGURATION SETTINGS
# ============================================================================

# CSV file directories - list format allows multiple metadata blocks
# Example: ['path/to/citation.csv', 'path/to/social_science.csv']
file_directory = ['DIRECTORY/FOR/Citation Metadata.csv', 'DIRECTORY/FOR/Social Science and Humanities.csv']

# API credentials and endpoint configuration
api_token_origin = "API KEY HERE"           # Dataverse API token
url_base_origin = 'BASE URL HERE'           # Dataverse base URL (e.g., https://demo.borealisdata.ca)

# API headers and client initialization
headers_origin = {'X-Dataverse-key': api_token_origin}
api_origin = NativeApi(url_base_origin, api_token_origin)
data_api_origin = DataAccessApi(url_base_origin, api_token_origin)


# ============================================================================
# CORE FUNCTIONS
# ============================================================================

def check_lock(dataset_id):
    """
    Check if a dataset is locked and wait for lock to be released.

    Dataset locks can occur during various operations like file ingestion or
    publishing. This function polls the lock status until the dataset is unlocked.

    Args:
        dataset_id (str): The ID of the dataset to check

    Returns:
        bool: True if dataset is unlocked, False if an error occurred

    Note:
        Documentation on dataset locks:
        https://guides.dataverse.org/en/latest/api/native-api.html#dataset-locks
    """
    time_start = datetime.now()
    print("Start check_lock")

    try:
        url = f"{url_base_origin}/api/datasets/{dataset_id}/locks"
        lock = requests.get(url, headers=headers_origin)

        if lock.status_code == 503:
            print("503 - Server is unavailable")
            sys.exit()

        attempt_count = 0
        while len(lock.json()['data']) > 0:
            print(f"Lock {attempt_count} times {dataset_id} {lock.json()}")
            print(lock.json())
            time.sleep(10)
            attempt_count += 1

            lock = requests.get(url, headers=headers_origin)
            if lock.status_code == 503:
                print("503 - Server is unavailable")
                sys.exit()

            if lock.status_code != 200:
                print(f"check_lock func: lock status {lock.status_code} for {dataset_id}")
                return False

    except Exception as e:
        print(f"check_lock Error: {str(e)}, dataset {dataset_id}")
        return False

    time_end = datetime.now()
    elapsed_time = (time_end - time_start).total_seconds()
    print(f"Dataset {dataset_id} was locked {elapsed_time} sec")

    return True


def update_metadata(latest_version, row, doi, header, directory, master_list, block):
    """
    Update dataset metadata by parsing CSV row values and pushing changes via API.

    This function processes metadata fields from a CSV row, formats them appropriately
    (primitive or compound), and sends updates to the Dataverse API.

    Args:
        latest_version (dict): Latest version metadata from Dataverse
        row (dict): Current CSV row containing update values
        doi (str): Dataset DOI
        header (list): CSV column headers
        directory (dict): Field definitions for the metadata block
        master_list (list): Contains [primitive_fields, compound_fields, controlled_vocab_fields]
        block (str): Metadata block name (e.g., 'citation', 'socialscience')
    """
    metadata_blocks = latest_version['metadataBlocks']
    fields = metadata_blocks[block]['fields']

    print('%%%%%%%%%%%%%%%%%%%%%%%%%%%%')
    print(fields)
    print('%%%%%%%%%%%%%%%%%%%%%%%%%%%%')

    existing_fields = [field for field in fields]
    existing_field_names = [field['typeName'] for field in fields]
    print(existing_fields)
    print(existing_field_names)

    field_index = 0
    for change_area in header:
        field_name = change_area.split(":")[0]
        print(field_name)

        # Handle new fields not in existing record
        if field_name not in existing_field_names and field_name != 'doi':
            print(f"{field_name} -- NOT IN EXISTING RECORD")

            if row[change_area] != '':
                print(f'RECORD TO ADD: {row[change_area]}')
                field = directory[field_name]

                # Process primitive fields
                if field_name in master_list[0]:
                    output = primitive_formatter(change_area, row, field)
                    print(f'OUTPUT: {output}')

                    if output == '':
                        field_index += 1
                        continue
                    else:
                        API_push(field, doi)

                # Process compound fields
                elif field_name in master_list[1]:
                    output = compound_formatter(change_area, row)

                    if output == False:
                        field_index += 1
                        continue
                    else:
                        field['value'] = output
                        print(field)
                        API_push(field, doi)
            else:
                print('-- NO RECORD TO ADD --')
                continue

        # Handle existing fields
        elif field_name in existing_field_names:
            current_field = existing_fields[field_index]
            print(f'Change_area = {change_area}')
            print(f'Fields = {current_field["typeName"]}')

            # Update primitive fields
            if current_field['typeName'] == change_area and current_field['typeName'] in master_list[0]:
                output = primitive_formatter(change_area, row, current_field)
                print(f'OUTPUT: {output}')

                if output == '':
                    field_index += 1
                    continue
                else:
                    API_push(current_field, doi)

            # Update compound fields
            elif current_field['typeName'] in change_area and current_field['typeName'] in master_list[1]:
                field_format = compound_formatter(change_area, row)

                if field_format == False:
                    field_index += 1
                    continue
                else:
                    if current_field['multiple']:
                        current_field['value'] = field_format
                    else:
                        current_field['value'] = field_format[0]

                    API_push(current_field, doi)

            field_index += 1


def primitive_formatter(change_area, row, field):
    """
    Format primitive metadata field values for updates.

    Handles single or multiple values, including splitting on '+' delimiter
    for multiple values in a single CSV cell.

    Args:
        change_area (str): Name of the field being updated
        row (dict): CSV row containing the new values
        field (dict): Field definition from the directory

    Returns:
        dict or str: Formatted field ready for API update
    """
    current_value = field['value']
    print(f"Unit to replace = {current_value}")

    # Handle multiple values separated by '+'
    if '+' in row[change_area]:
        new_values = row[change_area].split('+')
        print(f'New unit (plural): {new_values}')
        return record_check(new_values, field)

    # Handle single value
    else:
        if field['multiple']:
            new_values = [row[change_area]]
            print(f'New unit (singular): {new_values}')
            return record_check(new_values, field)
        else:
            new_value = row[change_area]
            print(f'New unit (singular): {new_value}')
            return record_check(new_value, field)


def record_check(new_value, field):
    """
    Validate and assign new value to field.

    Helper function to avoid code duplication in primitive_formatter.
    Checks if the new value is empty and handles field assignment.

    Args:
        new_value: The new value to assign (string or list)
        field (dict): Field dictionary to update

    Returns:
        dict or str: Updated field or empty string if no update needed
    """
    if new_value == [''] or new_value == '':
        print('NOT UPDATED IN THE RECORD')
        return ''
    else:
        field['value'] = new_value
        return field


def compound_formatter(header, row):
    """
    Format compound metadata field values for updates.

    Compound fields contain multiple primitive sub-fields. This function
    parses the header to identify sub-fields and formats the values accordingly.

    Args:
        header (str): CSV column header containing field and sub-field definitions
        row (dict): CSV row with the values to update

    Returns:
        list or bool: List of formatted compound values, or False if no update needed
    """
    field_parts = header.split(":")

    # Extract primitive fields from compound field header
    if ';' in field_parts[1]:
        primitive_fields = [field.strip() for field in field_parts[1].split(';')]
    else:
        primitive_fields = [field_parts[1].strip()]

    compound_dict = {field_parts[0]: primitive_fields}
    print(f"Citation dictionary = {compound_dict}")
    print(f'Compound list = {primitive_fields}')

    # Parse values from CSV row
    if row[header] != '':
        if '+' in row[header]:
            value_parts = row[header].split('+')
            value_list = []
            for part in value_parts:
                values = part.split(';')
                value_list.append(values)
        else:
            value_list = row[header].split(';')

        print(f"Value list = {value_list}")
    else:
        value_list = []

    # Format compound field values
    result_list = []
    counter = 0

    for field_type in compound_dict[field_parts[0]]:
        if counter >= len(value_list):
            break

        if value_list == [''] or value_list == []:
            counter += 1
            continue

        temp_dict = {}
        inner_counter = 0

        for item in value_list:
            value_setup = {}

            if isinstance(item, list):
                # Handle multiple primitive values within compound field
                inner_count = 0
                for sub_item in item:
                    sub_item = sub_item.strip()
                    primitive_dict = {
                        'typeName': primitive_fields[inner_count].strip(),
                        'multiple': False,
                        'typeClass': "primitive",
                        'value': sub_item
                    }
                    value_setup[primitive_fields[inner_count].strip()] = primitive_dict
                    inner_count += 1
            else:
                # Handle single primitive value
                item = item.strip()
                primitive_dict = {
                    'typeName': primitive_fields[inner_counter].strip(),
                    'multiple': False,
                    'typeClass': "primitive",
                    'value': item
                }
                temp_dict[primitive_fields[inner_counter].strip()] = primitive_dict
                inner_counter += 1

            if value_setup:
                result_list.append(value_setup)

        if not value_setup:
            result_list.append(temp_dict)

        counter += 1

    if not result_list:
        print("NOT UPDATED IN THE RECORD")
        return False

    return result_list


def file_loader():
    """
    Main entry point for processing CSV files and updating dataset metadata.

    Iterates through configured CSV files, reads each row, and updates
    corresponding dataset metadata in Dataverse.
    """
    for csv_path in file_directory:
        df = pd.read_csv(csv_path)
        headers = list(df.columns)
        print(headers)

        # Get metadata block configuration
        block_info = xml_selecter(headers)
        field_directory = block_info[0]
        block_name = block_info[1]
        master_lists = block_info[2]

        with open(csv_path, newline='', encoding='utf-8-sig') as csvfile:
            reader = csv.DictReader(csvfile)

            for row in reader:
                print(row)
                doi = row[headers[0]]

                # Standardize DOI format
                if 'https://doi.org/' in doi:
                    doi = doi.replace('https://doi.org/', 'doi:')
                print(doi)

                resp = api_origin.get_dataset(doi, version="2.0")
                print(resp.json())

                if resp.status_code == 200:
                    dataset_id = resp.json()['data']['id']
                    latest_version = resp.json()['data']['latestVersion']
                    print(latest_version)

                    # Update metadata
                    update_metadata(latest_version, row, doi, headers, field_directory, master_lists, block_name)

                    # Optional: Auto-publish dataset
                    # publish_dataset(doi)


def xml_selecter(headers):
    """
    Select and configure metadata block based on CSV headers.

    Determines which metadata block (citation, socialscience, etc.) to use
    based on markers in the CSV headers and returns the corresponding
    field definitions and configurations.

    Args:
        headers (list): CSV column headers

    Returns:
        list: [field_directory, block_name, master_lists]
    """
    # Citation metadata block configuration
    if 'MARKER 1' in headers:
        field_directory = {
            'title': {"typeName": "title", "multiple": False, "typeClass": "primitive", "value": ""},
            'subtitle': {"typeName": "subtitle", "multiple": False, "typeClass": "primitive", "value": ""},
            'alternativeTitle': {"typeName": "alternativeTitle", "multiple": True, "typeClass": "primitive", "value": [""]},
            'otherId': {"typeName": "otherId", "multiple": True, "typeClass": "compound", "value": [""]},
            'author': {"typeName": "author", "multiple": True, "typeClass": "compound", "value": [""]},
            'datasetContact': {"typeName": "datasetContact", "multiple": True, "typeClass": "compound", "value": [""]},
            'dsDescription': {"typeName": "dsDescription", "multiple": True, "typeClass": "compound", "value": [""]},
            'subject': {"typeName": "subject", "multiple": True, "typeClass": "controlledVocabulary", "value": [""]},
            'keyword': {"typeName": "keyword", "multiple": True, "typeClass": "compound", "value": [""]},
            'topicClassification': {"typeName": "topicClassification", "multiple": True, "typeClass": "compound", "value": [""]},
            'publication': {"typeName": "publication", "multiple": True, "typeClass": "compound", "value": [""]},
            'notesText': {"typeName": "notesText", "multiple": False, "typeClass": "primitive", "value": ""},
            'language': {"typeName": "language", "multiple": True, "typeClass": "controlledVocabulary", "value": [""]},
            'producer': {"typeName": "producer", "multiple": True, "typeClass": "compound", "value": [""]},
            'productionDate': {"typeName": "productionDate", "multiple": False, "typeClass": "primitive", "value": ""},
            'productionPlace': {"typeName": "productionPlace", "multiple": True, "typeClass": "primitive", "value": [""]},
            'contributor': {"typeName": "contributor", "multiple": True, "typeClass": "compound", "value": [""]},
            'grantNumber': {"typeName": "grantNumber", "multiple": True, "typeClass": "compound", "value": [""]},
            'distributor': {"typeName": "distributor", "multiple": True, "typeClass": "compound", "value": [""]},
            'distributionDate': {"typeName": "distributionDate", "multiple": False, "typeClass": "primitive", "value": ""},
            'depositor': {"typeName": "depositor", "multiple": False, "typeClass": "primitive", "value": ""},
            'dateOfDeposit': {"typeName": "dateOfDeposit", "multiple": False, "typeClass": "primitive", "value": ""},
            'timePeriodCovered': {"typeName": "timePeriodCovered", "multiple": True, "typeClass": "compound", "value": [""]},
            'dateOfCollection': {"typeName": "dateOfCollection", "multiple": True, "typeClass": "compound", "value": [""]},
            'kindOfData': {"typeName": "kindOfData", "multiple": True, "typeClass": "primitive", "value": [""]},
            'series': {"typeName": "series", "multiple": True, "typeClass": "compound", "value": [""]},
            'software': {"typeName": "software", "multiple": True, "typeClass": "compound", "value": [""]},
            'relatedMaterial': {"typeName": "relatedMaterial", "multiple": True, "typeClass": "primitive", "value": [""]},
            'relatedDatasets': {"typeName": "relatedDatasets", "multiple": True, "typeClass": "primitive", "value": [""]},
            'otherReferences': {"typeName": "otherReferences", "multiple": True, "typeClass": "primitive", "value": [""]},
            'dataSources': {"typeName": "dataSources", "multiple": True, "typeClass": "primitive", "value": [""]},
            'originOfSources': {"typeName": "originOfSources", "multiple": False, "typeClass": "primitive", "value": ""},
            'characteristicOfSources': {"typeName": "characteristicOfSources", "multiple": False, "typeClass": "primitive", "value": ""},
            'accessToSources': {"typeName": "accessToSources", "multiple": False, "typeClass": "primitive", "value": ""}
        }
        block_name = 'citation'

    # Social science metadata block configuration
    elif 'MARKER 2' in headers:
        field_directory = {
            'unitOfAnalysis': {"typeName": "unitOfAnalysis", "multiple": True, "typeClass": "primitive", "value": [""]},
            'universe': {"typeName": "universe", "multiple": True, "typeClass": "primitive", "value": [""]},
            'timeMethod': {"typeName": "timeMethod", "multiple": False, "typeClass": "primitive", "value": ""},
            'dataCollector': {"typeName": "dataCollector", "multiple": False, "typeClass": "primitive", "value": ""},
            'collectorTraining': {"typeName": "collectorTraining", "multiple": False, "typeClass": "primitive", "value": ""},
            'frequencyOfDataCollection': {"typeName": "frequencyOfDataCollection", "multiple": False, "typeClass": "primitive", "value": ""},
            'samplingProcedure': {"typeName": "samplingProcedure", "multiple": False, "typeClass": "primitive", "value": ""},
            'targetSampleSize': {"typeName": "targetSampleSize", "multiple": False, "typeClass": "compound", "value": ['']},
            'deviationsFromSampleDesign': {"typeName": "deviationsFromSampleDesign", "multiple": False, "typeClass": "primitive", "value": ""},
            'collectionMode': {"typeName": "collectionMode", "multiple": True, "typeClass": "primitive", "value": [""]},
            'researchInstrument': {"typeName": "researchInstrument", "multiple": False, "typeClass": "primitive", "value": ""},
            'dataCollectionSituation': {"typeName": "dataCollectionSituation", "multiple": False, "typeClass": "primitive", "value": ""},
            'actionsToMinimizeLoss': {"typeName": "actionsToMinimizeLoss", "multiple": False, "typeClass": "primitive", "value": ""},
            'controlOperations': {"typeName": "controlOperations", "multiple": False, "typeClass": "primitive", "value": ""},
            'weighting': {"typeName": "weighting", "multiple": False, "typeClass": "primitive", "value": ""},
            'cleaningOperations': {"typeName": "cleaningOperations", "multiple": False, "typeClass": "primitive", "value": ""},
            'datasetLevelErrorNotes': {"typeName": "datasetLevelErrorNotes", "multiple": False, "typeClass": "primitive", "value": ""},
            'responseRate': {"typeName": "responseRate", "multiple": False, "typeClass": "primitive", "value": ""},
            'samplingErrorEstimates': {"typeName": "samplingErrorEstimates", "multiple": False, "typeClass": "primitive", "value": ""},
            'otherDataAppraisal': {"typeName": "otherDataAppraisal", "multiple": False, "typeClass": "primitive", "value": ""},
            'socialScienceNotes': {"typeName": "socialScienceNotes", "multiple": False, "typeClass": "compound", "value": ['']}
        }
        block_name = 'socialscience'

    # Build master lists of field types
    master_lists = []
    primitive_fields = []
    compound_fields = []
    controlled_vocab_fields = []

    for field_name, field_def in field_directory.items():
        if field_def['typeClass'] == 'primitive':
            primitive_fields.append(field_name)
        elif field_def['typeClass'] == 'compound':
            compound_fields.append(field_name)
        elif field_def['typeClass'] == 'controlledVocabulary':
            controlled_vocab_fields.append(field_name)

    master_lists = [primitive_fields, compound_fields, controlled_vocab_fields]
    print(master_lists)

    return [field_directory, block_name, master_lists]


def API_push(field, doi):
    """
    Push metadata updates to the Dataverse API.

    Sends a PUT request to update dataset metadata with the provided field changes.

    Args:
        field (dict): Field data to update
        doi (str): Dataset DOI
    """
    print(json.dumps(field))
    url = f'{url_base_origin}/api/datasets/:persistentId/editMetadata?persistentId={doi}&replace=true'
    print(url)

    resp = requests.put(url, data=json.dumps(field), headers=headers_origin)
    print(resp.json())
    print(resp.status_code)
    print()


def publish_dataset(doi):
    """
    Publish dataset with a minor version increment.

    Optional function to automatically publish datasets after metadata updates.

    Args:
        doi (str): Dataset DOI

    Returns:
        int: HTTP status code from the publish operation
    """
    resp = api_origin.publish_dataset(doi, "minor")
    return resp.status_code


# ============================================================================
# SCRIPT EXECUTION
# ============================================================================

if __name__ == "__main__":
    file_loader()
