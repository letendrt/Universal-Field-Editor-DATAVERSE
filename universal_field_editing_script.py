# -*- coding: utf-8 -*-
"""Universal Field Editing Script.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1lyZL1iK1TS6-hUZlgMCrPUlnzqnrSJ0q

# UNIVERSAL SCRIPT EDITING
"""

# Library Import

#!pip install pyDataverse

from datetime import datetime
import sys
import time
import csv
import json
import pandas as pd


# Importing libraries for dataverse api
# Documentation found here: https://pydataverse.readthedocs.io/en/latest/
import requests
import pyDataverse.utils as utils
from pyDataverse.api import NativeApi, DataAccessApi

# API and URL configuration

api_token_origin = "a210b72e-3249-4a37-a4bb-88841f956cbd"                          # Enter API key as string
url_base_origin = 'https://demo.borealisdata.ca'                                   # Input base origin url as string

headers_origin = {'X-Dataverse-key': api_token_origin}                             # Create dictionary and insert API token as the value
api_origin = NativeApi(url_base_origin, api_token_origin)                          # API call using the pyDataverse
data_api_origin = DataAccessApi(url_base_origin, api_token_origin)

# This function checks if a dataset is locked (for one reason or another; it could be due to .tab file ingestion, for instance)
# More documentation on what this entails available here: https://guides.dataverse.org/en/6.2/api/native-api.html#dataset-locks

def check_lock(dataset_id):

    time_start = datetime.now()                                                     # Set up internal timer
    print("Start check_lock")                                                       # Print start message


    try:                                                                            # Checking if url is accessible.
        url = f"{url_base_origin}/api/datasets/{dataset_id}/locks"                  # Set up URL for access
        lock = requests.get(url, headers_origin)                                    # Set up lock status

        if lock.status_code == 503:                                                 # If URL is unavailable
            print("503 - Server is unavailable")                                    # Print status
            sys.exit()                                                              # stop running the function

        a = 0                                                                       # Starting attempt tracker
        while len(lock.json()['data']) > 0:                                         # If the tracker is above 0 (meaning dataset is locked)
            print(f"Lock {str(a)} times {dataset_id} {lock.json()}")                # API returns jason file of all locked datasets
            print(lock.json())                                                      # Print lock information
            time.sleep(10)                                                          # Start a 10 second timer
            a += 1                                                                  # Update attempt tracker

            lock = requests.get(url, headers_origin)                                # Check lock status
            if lock.status_code == 503:                                             # If URL is unavailable
                print("503 - Server is unavailable")                                # Print status
                sys.exit()                                                          # Stop running the function

            if lock.status_code != 200:                                                                 # If URL status code is not 200 (not successful)
                print(f"check_lock func: lock status {str(lock.status_code)} for {dataset_id}")         # Print lock information
                return False                                                                            # Return False

    except Exception as e:                                                          # This exception parameter prevents unwanted crashes or errors.
        print(f"check_lock. Error {str(e)}, dataset {dataset_id} ")                 # Print lock information
        return False                                                                # Return False

    time_end = datetime.now()                                                       # Provide date and time
    t = (time_end - time_start)                                                     # Give total time taken to ingest dataset
    print(f"Dataset {str(dataset_id)} was locked {str(t.total_seconds())} sec")     # Provide dataset information

    return True                                                                     # Return True

def update_metadata_name(latest_version, row, doi, header, directory, master_list):

    updated_citation = {}                                                            # Creating empty dictionary for indexing (I am aware that this is json formatting as well as dictionary)
    updated_citation['fields'] = []                                                  # Creating new dictionary lock for keywords
    updated_series = {}                                                              # Creating empty dictionary for indexing (I am aware that this is json formatting as well as dictionary)
    metadataBlocks_citation = latest_version['metadataBlocks']                       # Retrieving metadata for the dataset and assining it to the variable metadataBlocks
    fields = metadataBlocks_citation['citation']['fields']                           # Retrieving metadata fields from the extracted metadata blocks

    print('%%%%%%%%%%%%%%%%%%%%%%%%%%%%')
    print(fields)                                                                    # Prints all fields from the existing citation metadata blocks
    print('%%%%%%%%%%%%%%%%%%%%%%%%%%%%')

    existing_fields = [i for i in fields]                                            # Lists all the existing fields from the current record
    existing_field_typeNames = [i['typeName'] for i in fields]                       # Lists the name of all the existing fields from the current record
    print(existing_fields)
    print(existing_field_typeNames)


    root_counter = 0                                                                 # Setting up counter used in list parsing
    for change_area in header:                                                       # For all items in metadata fields
        assess = change_area.split(":")                                              # Split the header at the colon (item on the right of the column is the field typeName)
        print(assess[0])                                                             # Print the field typeName

        if assess[0] not in existing_field_typeNames and assess[0] != 'doi':         # If the existing record does not hold the current field typeName (and is not a DOI)
            print(f"{assess[0]} -- NOT IN EXISTING RECORD")                          # Indicate that the metadata field is not in the current record

            if row[change_area] != '':                                               # If the CSV cell associated to this field is not empty
                print(f'RECORD TO ADD: {row[change_area]}')                          # Print the content of the cell (the to-be added value)

                field = directory[assess[0]]                                         # Assigning respective directory value to the "field" variable (see file_loader() function for directory dictionary)

                if assess[0] in master_list[0]:                                      # If the typeName is in master_list[0] (meaning it is primitive)
                    output = primitive_formatter(change_area, row, field)            # Assign function output to the "output" variable
                    print(f'OUTPUT: {output}')

                    if output == '':                                                 # If the output is an empty string (meaning there is nothing to update the record with - shouldn't really happen here)
                        continue                                                     # Continue to next loop item

                    else:                                                            # Otherwise
                        API_push(field, doi)                                         # Push metadata changes to API

                if assess[0] in master_list[1]: # If
                    output = compound_formatter(change_area, row)

                    if output == False:
                        print()
                        continue

                    else:
                        field['value'] = output
                        print(field)
                        API_push(field, doi)

            else:
                print('-- NO RECORD TO ADD --')
                print()
                continue

        elif assess[0] in existing_field_typeNames:

            fields = existing_fields[root_counter]
            print(f'Change_area = {change_area}')
            print(f'Fields = {fields['typeName']}')

            if fields['typeName'] == change_area and fields['typeName'] in master_list[0]:                                           # If the field is that of the title

                field = existing_fields[root_counter]
                output = primitive_formatter(change_area, row, field)
                print(f'OUTPUT: {output}')

                if output == '':
                    continue

                else:
                    API_push(field, doi)


            if fields['typeName'] in change_area and fields['typeName'] in master_list[1]:
                field_format = compound_formatter(change_area, row)
                field = existing_fields[root_counter]


                if field_format == False:
                    print()
                    continue

                else:
                    field['value'] = field_format                                                   # Inserting new entry in previous value field
                    API_push(field, doi)

            root_counter += 1

def primitive_formatter(change_area, row, field):

    unit_to_replace = field['value']                                                 # Geographic data only has 2 entries for our purposes, which is why an index is used, there is a more efficient way of doing this I'm sure.
    print(f"Unit to replace = {unit_to_replace}")                                                          # Print fetched content

    if ';' in row[change_area]:
        new_unit = row[change_area].split(';')
        print(f'New unit (plural): {new_unit}')                                       # Printing new geographic unit

        if new_unit == [''] or new_unit == '':
            print('NOT UPDATED IN THE RECORD')
            record = ''
            print()
            return record

        else:
            field['value'] = new_unit                                                   # Inserting new entry in previous value field
            return field

    else:
        if field['multiple'] == True:
            new_unit = [row[change_area]]

        else:
            new_unit = row[change_area]

            print(f'New unit (singular): {new_unit}')

            if new_unit == [''] or new_unit == '':
                print('NOT UPDATED IN THE RECORD')
                record = ''
                print()
                return record

            else:
                field['value'] = new_unit                                                   # Inserting new entry in previous value field
                return field

def compound_formatter(header, row):

    first_list = header.split(":")

    if ';' in first_list[1]:
        compound_list = [i.strip() for i in first_list[1].split(';')]

    else:
        compound_list = [first_list[1].strip()]


    citation_dictionary = {}
    citation_dictionary[f'{first_list[0]}'] = compound_list
    print(f"Citation dictionary = {citation_dictionary}")
    print(f'Compound list = {compound_list}')


    if row[header] != '':

        if '+' in row[header]:
            val_list = row[header].split('+')
            value_list = []
            for i in val_list:
                value = i.split(';')
                value_list.append(value)

        else:
            value_list = row[header].split(';')

        print(f"Value list = {value_list}")

    else:
        value_list = []

    counter = 0
    list_of_list = []

    for types in citation_dictionary[f'{first_list[0]}']:
        populated_dictionary = {}

        if counter >= len(value_list):
            break

        if value_list == ['']:
          counter += 1
          continue

        else:
            list_of_list = []
            counter_2 = 0

            for items in value_list:
                val_setup = {}

                if type(items) is list:
                    count = 0
                    for i in items:
                        i = i.strip()
                        primitive_dictionary = {}
                        primitive_dictionary['typeName'] = compound_list[count].strip()
                        primitive_dictionary['multiple'] = False
                        primitive_dictionary['typeClass'] = "primitive"
                        primitive_dictionary['value'] = i

                        val_setup[compound_list[count].strip()] = primitive_dictionary
                        count += 1

                else:
                    items = items.strip()
                    primitive_dictionary = {}
                    primitive_dictionary['typeName'] = compound_list[counter_2].strip()
                    primitive_dictionary['multiple'] = False
                    primitive_dictionary['typeClass'] = "primitive"
                    primitive_dictionary['value'] = items

                    populated_dictionary[compound_list[counter_2].strip()] = primitive_dictionary
                    #print(populated_dictionary)

                    counter_2 += 1

                if val_setup != {}:
                  list_of_list.append(val_setup)

        if val_setup == {}:
            list_of_list.append(populated_dictionary)

        counter += 1


    if list_of_list == []:
        print("NOT UPDATED IN THE RECORD")
        return False

    else:
        return list_of_list

# File load and selection

def file_loader():

    df = pd.read_csv('/Citation Fields CSV - Feuille 1.csv')
    header = list(df.columns)
    print(header)

    #primitive_list = ['doi', 'subject', 'language', 'title', 'subtitle', 'alternativeTitle', 'notesText', 'productionDate', 'productionPlace', 'distributionDate', 'depositor', 'dateOfDeposit', 'kindOfData', 'relatedMaterials', 'relatedDatasets', 'otherReferences', 'dataSources', 'originOfSources', 'characteristicOfSources', 'accessToSources']
    #compound_list = ['otherId: otherIdAgency', 'author: authorName; authorAffiliation','keyword: keywordValue; keywordVocabulary', 'datasetContact: datasetContactName; datasetContactAffiliation', 'dsDescription: dsDescriptionValue; dsDescriptionDate', 'topicClassification: topicClassValue; topicClassVocab','publication: publicationRelationType; publicationCitation; publicationIDType; publicationIDNumber', 'producer: producerName; producerAffiliation; producerAbbreviation', 'contributor: contributorType; contributorName', 'grantNumber: grantNumberAgency; grantNumberValue', 'distributor: distributorName; distributorAffiliation; distributorAbbreviation; distributorURL', 'timePeriodCovered: timePeriodCoveredStart; timePeriodCoveredEnd', 'dateOfCollection: dateOfCollectionStart; dateOfCollectionEnd', 'series: seriesName; seriesInformation', 'software: softwareName']

    directory = {'title': {"typeName": "title", "multiple": False, "typeClass": "primitive", "value": ""},
             'subtitle': {"typeName": "subtitle", "multiple": False, "typeClass": "primitive", "value": ""},
             'alternativeTitle': {"typeName": "alternativeTitle", "multiple": True, "typeClass": "primitive", "value": [""]},
             'otherId': {"typeName": "otherId", "multiple": True, "typeClass": "compound", "value": [""]},
             'author': {"typeName": "author", "multiple": True, "typeClass": "compound", "value": [""]},
             'datasetContact': {"typeName": "datasetContact", "multiple": True, "typeClass": "compound", "value": [""]},
             'dsDescription': {"typeName": "dsDescription", "multiple": True, "typeClass": "compound", "value": [""]},
             'subject': {"typeName": "subject", "multiple": True, "typeClass": "controlledVocabulary", "value": [""]},
             'keyword': {"typeName": "keyword", "multiple": True, "typeClass": "compound", "value": [""]},
             'topicClassification': {"typeName": "topicClassification", "multiple": True, "typeClass": "compound", "value": [""]},
             'publication': {"typeName": "publication", "multiple": True, "typeClass": "compound", "value": [""]},
             'notesText': {"typeName": "notesText", "multiple": False, "typeClass": "primitive", "value": ""},
             'language': {"typeName": "language", "multiple": True, "typeClass": "controlledVocabulary", "value": [""]},
             'producer': {"typeName": "producer", "multiple": True, "typeClass": "compound", "value": [""]},
             'productionDate': {"typeName": "productionDate", "multiple": False, "typeClass": "primitive", "value": ""},
             'productionPlace': {"typeName": "productionPlace", "multiple": True, "typeClass": "primitive", "value": [""]},
             'contributor': {"typeName": "contributor", "multiple": True, "typeClass": "compound", "value": [""]},
             'grantNumber': {"typeName": "grantNumber", "multiple": True, "typeClass": "compound", "value": [""]},
             'distributor': {"typeName": "distributor", "multiple": True, "typeClass": "compound", "value": [""]},
             'distributionDate': {"typeName": "distributionDate", "multiple": False, "typeClass": "primitive", "value": ""},
             'depositor': {"typeName": "depositor", "multiple": False, "typeClass": "primitive", "value": ""},
             'dateOfDeposit': {"typeName": "dateOfDeposit", "multiple": False, "typeClass": "primitive", "value": ""},
             'timePeriodCovered': {"typeName": "timePeriodCovered", "multiple": True, "typeClass": "compound", "value": [""]},
             'dateOfCollection': {"typeName": "dateOfCollection", "multiple": True, "typeClass": "compound", "value": [""]},
             'kindOfData': {"typeName": "kindOfData", "multiple": True, "typeClass": "primitive", "value": [""]},
             'series': {"typeName": "series", "multiple": True, "typeClass": "compound", "value": [""]},
             'software': {"typeName": "software", "multiple": True, "typeClass": "compound", "value": [""]},
             'relatedDatasets': {"typeName": "relatedDatasets", "multiple": True, "typeClass": "primitive", "value": [""]},
             'otherReferences': {"typeName": "otherReferences", "multiple": True, "typeClass": "primitive", "value": [""]},
             'dataSources': {"typeName": "dataSources", "multiple": True, "typeClass": "primitive", "value": [""]},
             'originOfSources': {"typeName": "originOfSources", "multiple": False, "typeClass": "primitive", "value": ""},
             'characteristicOfSources': {"typeName": "characteristicOfSources", "multiple": False, "typeClass": "primitive", "value": ""},
             'accessToSources': {"typeName": "accessToSources", "multiple": False, "typeClass": "primitive", "value": ""}
             }

    master_list = []
    primitive_list = []
    compound_list = []
    controlled_vocab_list = []

    for k, v in directory.items():
        if v['typeClass'] == 'primitive':
            primitive_list.append(k)
        elif v['typeClass'] == 'compound':
            compound_list.append(k)
        elif v['typeClass'] == 'controlledVocabulary':
            #controlled_vocab_list.append(k)
            primitive_list.append(k)

    master_list.append(primitive_list)
    master_list.append(compound_list)
    master_list.append(controlled_vocab_list)

    print(master_list)

    with (open('/Citation Fields CSV - Feuille 1.csv', newline='', encoding='utf-8-sig') as csvfile):
        reader = csv.DictReader(csvfile)

        for row in reader:
            print(row)
            doi = row[header[0]]

            if 'https://doi.org/' in doi:
              doi = doi.replace('https://doi.org/', 'doi:')
            print(doi)

            resp = api_origin.get_dataset(doi, version="2.0")
            print(resp.json())

            if resp.status_code == 200:                                             # If API call is successful (marker 200)
                id = resp.json()['data']['id']                                      # old dataset ID is assigned to variable id
                latest_version = resp.json()['data']['latestVersion']               # Latest old dataset version is set to variable list "latest_version"
                print(latest_version)

            pub_stat = update_metadata_name(latest_version, row, doi, header, directory, master_list)


            #if pub_stat == True:
                #publish_dataset(doi)

# Push to API

def API_push(field, doi):

    print(json.dumps(field))                                                                                # Print new metadata field (converts python dictionary to json format)
    url = f'{url_base_origin}/api/datasets/:persistentId/editMetadata?persistentId={doi}&replace=true'      # Update URL variable for API call
    print(url)                                                                                              # Print URL

    resp = requests.put(url, data=json.dumps(field), headers=headers_origin)                                # Upload the updated metadata fields after converting the dictionaries to json format
    print(resp.json())                                                                                      # Print metadata block in json format (?)
    print(resp.status_code)
    print()

# Publish Dataset

def publish_dataset(doi):
    resp = api_origin.publish_dataset(doi, "minor")
    return resp.status_code

# TEST ZONE

#### Backup
"""
def update_metadata_name(latest_version, row, doi, header, directory, master_list):

    updated_citation = {}                                                            # Creating empty dictionary for indexing (I am aware that this is json formatting as well as dictionary)
    updated_citation['fields'] = []                                                  # Creating new dictionary lock for keywords
    updated_series = {}                                                                # Creating empty dictionary for indexing (I am aware that this is json formatting as well as dictionary)
    metadataBlocks_citation = latest_version['metadataBlocks']                           # Retrieving metadata for the dataset and assining it to the variable metadataBlocks
    fields = metadataBlocks_citation['citation']['fields']                         # Retrieving metadata fields from the extracted metadata blocks

    print('%%%%%%%%%%%%%%%%%%%%%%%%%%%%')
    print(fields)
    print('%%%%%%%%%%%%%%%%%%%%%%%%%%%%')




    for field in fields:                                                           # For all items in metadata fields
        for change_area in header:

            if field['typeName'] == change_area and field['typeName'] in master_list[0]:                                           # If the field is that of the title

                unit_to_replace = field['value']                                                 # Geographic data only has 2 entries for our purposes, which is why an index is used, there is a more efficient way of doing this I'm sure.
                print(f"Unit to replace = {unit_to_replace}")                                                          # Print fetched content

                if ';' in row[change_area]:
                    new_unit = row[change_area].split(';')
                    print(f'New unit (plural): {new_unit}')                                       # Printing new geographic unit

                    if new_unit == [''] or new_unit == '':
                        print('NOT UPDATED IN THE RECORD')
                        print()
                        continue

                    else:
                        field['value'] = new_unit                                                   # Inserting new entry in previous value field
                        print(json.dumps(field))                                                  # Print new metadata field (converts python dictionary to json format)


                        url = f'{url_base_origin}/api/datasets/:persistentId/editMetadata?persistentId={doi}&replace=true'      # Update URL variable for API call
                        print(url)                                                                                              # Print URL

                        resp = requests.put(url, data=json.dumps(field), headers=headers_origin)         # Upload the updated metadata fields after converting the dictionaries to json format
                        print(resp.json())                                                                        # Print metadata block in json format (?)
                        print(resp.status_code)
                        print()

                else:

                    if field['multiple'] == True:
                        new_unit = [row[change_area]]

                    else:
                        new_unit = row[change_area]

                    print(f'New unit (singular): {new_unit}')

                    if new_unit == [''] or new_unit == '':
                        print('NOT UPDATED IN THE RECORD')
                        print()
                        continue

                    else:
                        field['value'] = new_unit                                                   # Inserting new entry in previous value field
                        print(json.dumps(field))                                                  # Print new metadata field (converts python dictionary to json format)

                        url = f'{url_base_origin}/api/datasets/:persistentId/editMetadata?persistentId={doi}&replace=true'      # Update URL variable for API call
                        print(url)                                                                                              # Print URL

                        resp = requests.put(url, data=json.dumps(field), headers=headers_origin)         # Upload the updated metadata fields after converting the dictionaries to json format
                        print(resp.json())                                                                        # Print metadata block in json format (?)
                        print(resp.status_code)
                        print()

            main_head = change_area.split(':')
            if field['typeName'] in change_area and main_head[0] in master_list[1]:
                field_format = compound_formatter(change_area, row)

                if field_format == False:
                    print()
                    continue

                else:
                    field['value'] = field_format                                                   # Inserting new entry in previous value field
                    print(json.dumps(field))                                                  # Print new metadata field (converts python dictionary to json format)


                    url = f'{url_base_origin}/api/datasets/:persistentId/editMetadata?persistentId={doi}&replace=true'      # Update URL variable for API call
                    print(url)                                                                                              # Print URL

                    resp = requests.put(url, data=json.dumps(field), headers=headers_origin)         # Upload the updated metadata fields after converting the dictionaries to json format
                    print(resp.json())                                                                        # Print metadata block in json format (?)
                    print(resp.status_code)
                    print()


            record_expansion()



######## DEPRECATED

def keyword_update(field, row, change_area):
    print(field)
    print(row)
    print(change_area)

    if ',' in row[change_area]:
        listed_items = row[change_area].split(',')
        print(listed_items)

        if field['multiple'] == False:
            field['multiple'] = True

        new_unit = listed_items

        dictionary_list = []
        for keyword in new_unit:

          keyword_dict = {}
          keyword_dict['typeName'] = 'keywordValue'
          keyword_dict['multiple'] = False
          keyword_dict['typeClass'] = 'primitive'
          keyword_dict["value"] = keyword

          val_setup = {}
          val_setup["keywordValue"] = keyword_dict

          dictionary_list.append(val_setup)


        print(f'New unit (plural): {new_unit}')                                       # Printing new geographic unit

    else:
        if field['multiple'] == True:
            field['multiple'] = False

        new_unit = row[change_area]
        dictionary_list = []

        keyword_dict = {}
        keyword_dict['typeName'] = 'keywordValue'
        keyword_dict['multiple'] = False
        keyword_dict['typeClass'] = 'primitive'
        keyword_dict["value"] = new_unit

        val_setup = {}
        val_setup["keywordValue"] = keyword_dict
        dictionary_list.append(val_setup)

    return dictionary_list

"""

file_loader()